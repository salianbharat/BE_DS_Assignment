{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOYhZ/gEggj+wqWyJKS1Hhk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/salianbharat/BE_DS_Assignment/blob/main/ObesityClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Introduction**\n",
        "\n",
        "\n",
        "## Problem Statement\n",
        "This notebook aims to explore and analyze the obesity classification dataset using Decision Tree algorithms. The objective is to build predictive models that can classify individuals into different obesity categories based on various attributes such as age, gender, family history, and lifestyle factors. By leveraging decision tree techniques, we seek to extract meaningful insights from the data and develop accurate predictive models to aid in obesity classification.\n",
        "\n",
        "## Dataset Overview\n",
        "The obesity classification dataset contains information about individuals' attributes and their corresponding obesity levels, categorized into four classes: Insufficient Weight, Normal Weight, Overweight Level I, Overweight Level II, and Obesity Type I, II, and III. The dataset includes features such as age, gender, height, weight, family history of obesity, dietary habits, physical activity level, and other lifestyle factors.\n",
        "\n",
        "## Approach\n",
        "1. **Foundational Knowledge:** We'll start by understanding the principles of decision trees and familiarizing ourselves with decision tree algorithms.\n",
        "2. **Data Exploration:** We'll analyze the dataset's structure and characteristics using various exploratory techniques such as histograms, scatter plots, and correlation matrices to gain insights into the dataset's attributes.\n",
        "3. **Preprocessing and Feature Engineering:** We'll handle missing values, encode categorical variables, and split the dataset into training and testing sets.\n",
        "4. **Decision Tree Construction:** We'll choose appropriate hyperparameters and implement decision tree algorithms to train the models on the training data.\n",
        "5. **Model Evaluation:** We'll evaluate the trained models using metrics such as accuracy, precision, recall, and F1-score, and visualize the decision trees to interpret the learned decision rules and feature importance.\n",
        "6. **Hyperparameter Tuning and Model Optimization:** We'll perform hyperparameter tuning and optimization to improve the model's performance using techniques like grid search or random search and validate the optimized model using cross-validation techniques.\n",
        "\n",
        "\n",
        "Let's begin by importing the necessary libraries and loading the dataset.\n"
      ],
      "metadata": {
        "id": "BngEnU6j2p6S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importing requied libraries**"
      ],
      "metadata": {
        "id": "kj0tGny05TxK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Pandas: For data manipulation and analysis\n",
        "import pandas as pd\n",
        "\n",
        "# Matplotlib and Seaborn: For data visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Scikit-learn: For building and evaluating decision tree models\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n"
      ],
      "metadata": {
        "id": "7FEIvGfl5k3W"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "GnBYAS1x2r0b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n"
      ],
      "metadata": {
        "id": "FuDjGOiR3CtS"
      },
      "execution_count": 6,
      "outputs": []
    }
  ]
}